# Deep Residual Learning for Image Recognition

## 问题

### 为什么深度卷积神经网络的不同layer能得到不同的特征

深度卷积神经网络（CNN）之所以能在不同层提取不同的特征，是因为每一层的感受野、卷积核作用和语义抽象程度都不同。

#### 不同层有不同的感受野（Receptive Field）

* 网络越往后，单个神经元对应原图更大区域的信息。

* 浅层：关注局部、低级特征，如边缘、纹理。

* 深层：关注更大的区域，学习出抽象、高级语义特征，如人脸、物体类别等。

##### 一、感受野（Receptive Field）

**定义：**
某一层中一个神经元所“看到”的输入图像的区域大小。随着网络深度增加，一个神经元的感受野也会变大。

**理解方式：**
想象你用一只小望远镜观察画布，低层神经元看的是像素级小区域，高层神经元通过多层叠加，可以“望”到整幅图像的大块区域。

**例子：**

* 第一层卷积核 3×3，感受野为 3×3。
* 经过 3 层卷积（都为 3×3），其最上层神经元感受到的原图区域为 7×7（不是简单加法，而是考虑叠加和步长等因素）。

**意义：**
较大的感受野使模型可以捕捉更全局的信息，适合理解高级语义，比如“整张猫脸”。


##### 二、卷积核作用（Convolutional Filters）

**定义：**
卷积核是一个小窗口，它在图像上滑动，提取局部特征。不同的卷积核学习不同的“图像特征”。

**理解方式：**

* 低层卷积核：通常学到边缘、角点、纹理。
* 中层卷积核：学到形状、轮廓、局部模式。
* 高层卷积核：学到物体部件、完整结构。

**例子：**

* 第一层可能学习横线（如猫须）、竖线（如树干）。
* 第三层可能组合这些，学到“眼睛”或“轮子”。
* 第七层可能学到“整只猫”或“汽车”。

**在 ResNet 中：**
例如 3×3 卷积可以形成感受野扩大的“模块”，而残差结构（如 `F(x) + x`）允许更深层有效学习这些组合特征。


##### 三、语义抽象程度（Semantic Abstraction）

**定义：**
层数越高，所提取的特征就越抽象，越接近人类语义理解。

**理解方式：**

* 低层理解的是“像素”或“线条”。
* 中层理解的是“形状”或“部位”。
* 高层理解的是“类别”或“概念”。

**例子：**

* 输入是一张狗的照片。
* 第1层输出类似“狗耳朵边缘”。
* 第5层可能输出“有耳朵、有眼睛、有毛发的东西”。
* 第10层最终输出“Golden Retriever”概率最高。

**在论文中体现：**
作者强调“very deep networks”能够整合 low/mid/high-level features，并在 ImageNet/COCO 等任务上取得突破性效果。


##### 总结类比：

| 层级   | 感受野 | 卷积核作用   | 语义抽象程度     |
| ---- | --- | ------- | ---------- |
| 第一层  | 小   | 边缘、颜色等  | 几乎没有语义     |
| 中间层  | 中   | 组合纹理、形状 | 局部语义，如“眼睛” |
| 最后一层 | 大   | 整体对象特征  | 高级语义，“狗”   |

> 我还是没有深度掌握CNN的工作原理，我只知道对于一个图像，我们先把其转化成一个矩阵，按照像素或者rag，然后呢？上面这些东西的工作能够实现是基于什么？

#### 多层非线性映射叠加，逐步逼近复杂函数

* 每一层通过 ReLU 等非线性激活函数变换后，能学习不同层次的函数表示。

* 浅层不能直接表示复杂语义，但深层可以组合已有特征构造更高级模式。

#### Residual Learning 帮助深层优化、特征更细致

* ResNet 使用残差连接，令每一组层学习残差函数 $F(x) = H(x) - x$，而不是直接逼近目标函数 $H(x)$。
* 这样优化更容易，允许堆叠更多层，从而提取更丰富的特征。
* 深层的残差响应一般更小，表示每层只需“微调”信息，逐步细化表示。

---

### ✅ 4. 不同层间的信息抽象与重用机制

* 特征图在每层通常通过卷积核通道数变化、池化等机制逐步压缩空间，增加通道深度。
* 后层能“重用”前层输出的结构信息，进一步加工为高层语义。

---

### 直观类比

可以将 CNN 理解为一个多层图像处理流水线：

* 前几层像是在模糊图像边界、突出线条。
* 中间几层则把这些线条组合成局部结构，如“眼睛”或“轮廓”。
* 后几层则识别出完整的语义对象，如“猫脸”或“汽车”。

---

如果你希望，我可以结合 ResNet 的结构图进一步可视化地解释每一层学到了什么特征。是否需要？


### 当你的网络堆得很深的时候，你的梯度要么出现爆炸，要么消失

 


