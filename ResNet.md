# Deep Residual Learning for Image Recognition

## 问题

### 为什么深度卷积神经网络的不同layer能得到不同的特征

深度卷积神经网络（CNN）之所以能在不同层提取不同的特征，是因为每一层的感受野、卷积核作用和语义抽象程度都不同。

#### 不同层有不同的感受野（Receptive Field）

* 网络越往后，单个神经元对应原图更大区域的信息。

* 浅层：关注局部、低级特征，如边缘、纹理。

* 深层：关注更大的区域，学习出抽象、高级语义特征，如人脸、物体类别等。

##### 一、感受野（Receptive Field）

**定义：**
某一层中一个神经元所“看到”的输入图像的区域大小。随着网络深度增加，一个神经元的感受野也会变大。

**理解方式：**
想象你用一只小望远镜观察画布，低层神经元看的是像素级小区域，高层神经元通过多层叠加，可以“望”到整幅图像的大块区域。

**例子：**

* 第一层卷积核 3×3，感受野为 3×3。
* 经过 3 层卷积（都为 3×3），其最上层神经元感受到的原图区域为 7×7（不是简单加法，而是考虑叠加和步长等因素）。

**意义：**
较大的感受野使模型可以捕捉更全局的信息，适合理解高级语义，比如“整张猫脸”。


##### 二、卷积核作用（Convolutional Filters）

**定义：**
卷积核是一个小窗口，它在图像上滑动，提取局部特征。不同的卷积核学习不同的“图像特征”。

**理解方式：**

* 低层卷积核：通常学到边缘、角点、纹理。
* 中层卷积核：学到形状、轮廓、局部模式。
* 高层卷积核：学到物体部件、完整结构。

**例子：**

* 第一层可能学习横线（如猫须）、竖线（如树干）。
* 第三层可能组合这些，学到“眼睛”或“轮子”。
* 第七层可能学到“整只猫”或“汽车”。

**在 ResNet 中：**
例如 3×3 卷积可以形成感受野扩大的“模块”，而残差结构（如 `F(x) + x`）允许更深层有效学习这些组合特征。


##### 三、语义抽象程度（Semantic Abstraction）

**定义：**
层数越高，所提取的特征就越抽象，越接近人类语义理解。

**理解方式：**

* 低层理解的是“像素”或“线条”。
* 中层理解的是“形状”或“部位”。
* 高层理解的是“类别”或“概念”。

**例子：**

* 输入是一张狗的照片。
* 第1层输出类似“狗耳朵边缘”。
* 第5层可能输出“有耳朵、有眼睛、有毛发的东西”。
* 第10层最终输出“Golden Retriever”概率最高。

**在论文中体现：**
作者强调“very deep networks”能够整合 low/mid/high-level features，并在 ImageNet/COCO 等任务上取得突破性效果。


##### 总结类比：

| 层级   | 感受野 | 卷积核作用   | 语义抽象程度     |
| ---- | --- | ------- | ---------- |
| 第一层  | 小   | 边缘、颜色等  | 几乎没有语义     |
| 中间层  | 中   | 组合纹理、形状 | 局部语义，如“眼睛” |
| 最后一层 | 大   | 整体对象特征  | 高级语义，“狗”   |

> 我还是没有深度掌握CNN的工作原理，我只知道对于一个图像，我们先把其转化成一个矩阵，按照像素或者rag，然后呢？上面这些东西的工作能够实现是基于什么？

#### 多层非线性映射叠加，逐步逼近复杂函数

* 每一层通过 ReLU 等非线性激活函数变换后，能学习不同层次的函数表示。

* 浅层不能直接表示复杂语义，但深层可以组合已有特征构造更高级模式。

 

**激活函数的种类、各自的特点、ReLU 的本质作用，以及它与数据预处理中“去掉负值”的区别：**


##### 一、常见的非线性激活函数及作用对比

| 名称         | 数学表达式                                     | 特点和作用                                |
| ---------- | ----------------------------------------- | ------------------------------------ |
| Sigmoid    | σ(x) = 1 / (1 + e^(−x))                   | 输出范围在 (0, 1)，适合二分类输出；容易梯度消失，训练深层网络困难 |
| Tanh       | tanh(x) = (e^x − e^(−x)) / (e^x + e^(−x)) | 输出范围在 (−1, 1)，零均值；也有梯度消失问题           |
| **ReLU**   | ReLU(x) = max(0, x)                       | 计算简单，不会饱和，能有效缓解梯度消失问题，是目前主流选择        |
| Leaky ReLU | x if x > 0 else αx（通常 α = 0.01）           | 允许负方向有微弱梯度，缓解“ReLU神经元死亡”问题           |
| ELU        | x if x > 0 else α(e^x − 1)                | 与 Leaky ReLU 相似，但负方向更平滑，训练时更稳定       |
| GELU       | x × Φ(x)，Φ为标准正态分布累计函数                     | 在 Transformer 中常用，融合概率思想，表现更好但计算较复杂  |
| Softplus   | log(1 + e^x)                              | 平滑版本的 ReLU，有数学可导性（连续可微）              |


##### 二、ReLU 的真正作用 —— 并不仅仅是“把负数变 0”

1. 非线性引入

ReLU 是非线性的，它允许神经网络拟合非线性函数（否则多层线性就是一层线性的等效），这是构建表达力的根本。

2. 稀疏激活（Sparse Activation）

它会产生大量 0，使得每层仅一部分神经元激活，类似大脑中某些神经元在特定刺激下才活跃，提高计算效率和泛化能力。（那些为负数，说明特征完全不符合，记为0）


3. 避免梯度消失（Compared with Sigmoid / Tanh）

* ReLU 的导数在正区间是常数 1，能保持反向传播的梯度不至于趋近于 0；
* 这使得 ReLU 在深层网络（如 ResNet）中更稳定可训练。


##### 三、与数据预处理中“去掉负值”的本质区别

问：**如果理论上数据一定为正，那遇到负值是不是就像 ReLU 一样设为 0？**

表面相似，但意义完全不同：

| 情境             | 目的            | 本质行为                   |
| -------------- | ------------- | ---------------------- |
| **数据预处理中去负值**  | 清理“错误”或“异常”数据 | 认为负值是无效的，进行人为规则式修正     |
| **ReLU 中设为 0** | 网络自主学习特征      | 是神经网络自动决定某些特征“不重要”而抑制它 |

> 换句话说：
>
> * 数据清洗是人为的先验判断；
> * ReLU 是模型内部的后验“自我筛选”。

例如，在图像处理中：

* 输入可能本来就为非负（0\~255 像素值）；
* 但经过卷积后会产生正负混合的特征图，ReLU 是在这些中间特征空间中进行筛选的，而不是对原始“脏数据”修补。

##### 四、举个例子直观理解 ReLU 的意义

假设我们训练一个模型识别“猫”：

* 第一层卷积检测“耳朵形状”；
* 如果某个区域恰好像“狗耳朵”，那么卷积值可能为负；
* ReLU 把它置为 0，表示“这个区域不符合猫耳朵的特征”，因此忽略它；
* 模型最终关注那些有积极响应的特征。

**ReLU 是对神经网络学到的“偏好”进行“正向过滤”的过程。**

##### 五、总结

| 问题               | 回答                                           |
| ---------------- | -------------------------------------------- |
| 非线性激活函数有哪些？      | Sigmoid, Tanh, ReLU, Leaky ReLU, ELU, GELU 等 |
| ReLU 真正作用是？      | 提供非线性表达、避免梯度消失、产生稀疏表示、抑制非关键特征                |
| 与“负数据设为 0”有什么不同？ | ReLU 是模型自我选择特征重要性，数据清洗是人为假设下的强制处理            |

> NOTE：常说的softmax函数用在输出层，ReLU用在隐藏层

##### 六、Softmax激活函数

* Softmax常用于**输出层**，尤其在**多分类任务中**扮演关键角色
* Softmax 的使命是把“模型分数”转化为“概率决策”。它不是为了提取特征，而是为了做出选择。

**数学表达式：**

对输入向量 \$\mathbf{z} = \[z\_1, z\_2, \dots, z\_K]\$，Softmax 的输出是一个概率分布：

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} \quad \text{for } i = 1,\dots,K
$$

输出满足：

* 所有值在 (0,1) 之间；
* 所有值之和为 1（因此可解释为概率）。


**Softmax 的作用**

| 功能           | 解释                                            |
| ------------ | --------------------------------------------- |
| **归一化为概率分布** | 将原始网络输出（可能为任意实数）变成概率形式，方便计算分类准确率与交叉熵损失。       |
| **放大差异**     | Softmax 对最大值更敏感，使得最大的输出更接近 1，其他更趋近 0，强化“选择性”。 |
| **可微分**      | 与反向传播兼容，是神经网络可训练的重要组成部分。                      |


**Softmax 与其他激活函数的不同**

| 特点   | Softmax            | ReLU / Sigmoid 等  |
| ---- | ------------------ | ----------------- |
| 使用位置 | 通常用在**输出层**        | 用在**隐藏层**         |
| 输出范围 | 多个值 ∈ (0,1)，且总和为 1 | 各值独立处理，可能在负数或其它范围 |
| 含语义  | 每个输出表示“属于某类别的概率”   | 没有概率语义，更多表示激活强度   |


**图像多分类**

假设模型最后一层输出是 `[2.0, 1.0, 0.1]`，表示：

* 类别 A 得分 2.0
* 类别 B 得分 1.0
* 类别 C 得分 0.1

使用 Softmax：

$$
\text{Softmax}([2.0, 1.0, 0.1]) \approx [0.65, 0.24, 0.11]
$$

* 模型预测“属于 A”的概率为 65%
* 用于计算损失函数（如 CrossEntropyLoss）时，Softmax + log 联合工作。

**Softmax 在 Transformer 中也很关键**

在自注意力机制（Self-Attention）中：

* Softmax 用于对“注意力得分”进行归一化处理；
* 表示“当前词对其他词的注意程度分布”。

**Softmax vs. Top-k / Argmax**

| 方法             | 是否可导 | 是否可训练 | 输出可解释性      |
| -------------- | ---- | ----- | ----------- |
| Softmax        | ✅ 是  | ✅ 是   | 概率形式，支持反向传播 |
| Argmax / Top-k | ❌ 否  | ❌ 否   | 硬选择（不可微）    |


#### Residual Learning 帮助深层优化、特征更细致

* ResNet 使用残差连接，令每一组层学习残差函数 $F(x) = H(x) - x$，而不是直接逼近目标函数 $H(x)$。
* 这样优化更容易，允许堆叠更多层，从而提取更丰富的特征。
* 深层的残差响应一般更小，表示每层只需“微调”信息，逐步细化表示。


#### 不同层间的信息抽象与重用机制

* 特征图在每层通常通过卷积核通道数变化、池化等机制逐步压缩空间，增加通道深度。
* 后层能“重用”前层输出的结构信息，进一步加工为高层语义。

#### 直观类比

可以将 CNN 理解为一个多层图像处理流水线：

* 前几层像是在模糊图像边界、突出线条。
* 中间几层则把这些线条组合成局部结构，如“眼睛”或“轮廓”。
* 后几层则识别出完整的语义对象，如“猫脸”或“汽车”。




### 当你的网络堆得很深的时候，你的梯度要么出现爆炸，要么消失

 


